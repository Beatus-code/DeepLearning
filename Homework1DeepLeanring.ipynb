{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Scaling \n",
    "<li>Mean Subtraction</li>\n",
    "<li>Normalization</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import libraries and Fashion mnist Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# load dataset  datasets..load_data()\n",
    "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Reshaping datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height, channels = trainX.shape[1], trainX.shape[2], 1\n",
    "trainX = trainX.reshape((trainX.shape[0], width, height, channels))\n",
    "testX = testX.reshape((testX.shape[0], width, height, channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: One hot encode and Calculating Mean using ImageDataGenerator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape dataset to have a single channel\n",
    "width, height, channels = trainX.shape[1], trainX.shape[2], 1\n",
    "trainX = trainX.reshape((trainX.shape[0], width, height, channels))\n",
    "testX = testX.reshape((testX.shape[0], width, height, channels))\n",
    "# one hot encode target values\n",
    "trainY = to_categorical(trainY)\n",
    "testY = to_categorical(testY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Creating Convolution Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanSubtraction():\n",
    "    # create generator to center images\n",
    "    datagen = ImageDataGenerator(featurewise_center=True)\n",
    "    # calculate mean on training dataset\n",
    "    datagen.fit(trainX)\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    test_iterator = datagen.flow(testX, testY, batch_size=64)\n",
    "    print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit model with generator\n",
    "    model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches train=938, test=157\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 1.5976 - accuracy: 0.7391\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.3555 - accuracy: 0.8707\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.3057 - accuracy: 0.8868\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2783 - accuracy: 0.8965\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2593 - accuracy: 0.9043\n",
      "Test Accuracy: 88.030\n"
     ]
    }
   ],
   "source": [
    "MeanSubtraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Normailization using informations in step 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalization():\n",
    "    # confirm scale of pixels\n",
    "    print('Train min=%.3f, max=%.3f' % (trainX.min(), trainX.max()))\n",
    "    print('Test min=%.3f, max=%.3f' % (testX.min(), testX.max()))\n",
    "    # create generator (1.0/255.0 = 0.003921568627451)\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    test_iterator = datagen.flow(testX, testY, batch_size=64)\n",
    "    print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "    # confirm the scaling works\n",
    "    batchX, batchy = train_iterator.next()\n",
    "    print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit model with generator\n",
    "    model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min=0.000, max=255.000\n",
      "Test min=0.000, max=255.000\n",
      "Batches train=938, test=157\n",
      "Batch shape=(64, 28, 28, 1), min=0.000, max=1.000\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.7182 - accuracy: 0.7399\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.3373 - accuracy: 0.8786\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2862 - accuracy: 0.8955\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.2577 - accuracy: 0.9046\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.2309 - accuracy: 0.9153\n",
      "Test Accuracy: 90.500\n"
     ]
    }
   ],
   "source": [
    "Normalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weights Initialization\n",
    "<li>Xavier Initialization</li>\n",
    "<li>He Initialization</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1: Xavier Initialization using Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def XavierInitialization():\n",
    "    # confirm scale of pixels\n",
    "    print('Train min=%.3f, max=%.3f' % (trainX.min(), trainX.max()))\n",
    "    print('Test min=%.3f, max=%.3f' % (testX.min(), testX.max()))\n",
    "    # create generator (1.0/255.0 = 0.003921568627451)\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    test_iterator = datagen.flow(testX, testY, batch_size=64)\n",
    "    print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "    # confirm the scaling works\n",
    "    batchX, batchy = train_iterator.next()\n",
    "    print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels),\n",
    "                    kernel_initializer = tf.keras.initializers.GlorotNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer = tf.keras.initializers.GlorotNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu',kernel_initializer = tf.keras.initializers.GlorotNormal()))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit model with generator\n",
    "    model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min=0.000, max=255.000\n",
      "Test min=0.000, max=255.000\n",
      "Batches train=938, test=157\n",
      "Batch shape=(64, 28, 28, 1), min=0.000, max=1.000\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.7049 - accuracy: 0.7541\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.3395 - accuracy: 0.8765\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2898 - accuracy: 0.8941\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2530 - accuracy: 0.9081\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2299 - accuracy: 0.9154\n",
      "Test Accuracy: 90.080\n"
     ]
    }
   ],
   "source": [
    "XavierInitialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: He  Initialization using Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HeInitialization():\n",
    "    # confirm scale of pixels\n",
    "    print('Train min=%.3f, max=%.3f' % (trainX.min(), trainX.max()))\n",
    "    print('Test min=%.3f, max=%.3f' % (testX.min(), testX.max()))\n",
    "    # create generator (1.0/255.0 = 0.003921568627451)\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    test_iterator = datagen.flow(testX, testY, batch_size=64)\n",
    "    print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "    # confirm the scaling works\n",
    "    batchX, batchy = train_iterator.next()\n",
    "    print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels),\n",
    "                    kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit model with generator\n",
    "    model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min=0.000, max=255.000\n",
      "Test min=0.000, max=255.000\n",
      "Batches train=938, test=157\n",
      "Batch shape=(64, 28, 28, 1), min=0.000, max=1.000\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.6084 - accuracy: 0.7819\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.3129 - accuracy: 0.8861\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2649 - accuracy: 0.9042\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2333 - accuracy: 0.9139\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2082 - accuracy: 0.9231\n",
      "Test Accuracy: 90.250\n"
     ]
    }
   ],
   "source": [
    "HeInitialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Configuration\n",
    "<li>1 input layer: 2 hidden Layer : 1 output Layer</li>\n",
    "<li>1 input layer: 3 hidden Layer : 1 output Layer</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1: 1 input layer: 2 hidden Layer : 1 output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Configuration1():\n",
    "    # confirm scale of pixels\n",
    "    print('Train min=%.3f, max=%.3f' % (trainX.min(), trainX.max()))\n",
    "    print('Test min=%.3f, max=%.3f' % (testX.min(), testX.max()))\n",
    "    # create generator (1.0/255.0 = 0.003921568627451)\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    test_iterator = datagen.flow(testX, testY, batch_size=64)\n",
    "    print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "    # confirm the scaling works\n",
    "    batchX, batchy = train_iterator.next()\n",
    "    print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels),\n",
    "                    kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit model with generator\n",
    "    model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min=0.000, max=255.000\n",
      "Test min=0.000, max=255.000\n",
      "Batches train=938, test=157\n",
      "Batch shape=(64, 28, 28, 1), min=0.000, max=1.000\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.6158 - accuracy: 0.7735\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.3220 - accuracy: 0.8826\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.2717 - accuracy: 0.8996\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.2344 - accuracy: 0.9120\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.2114 - accuracy: 0.9239\n",
      "Test Accuracy: 90.450\n"
     ]
    }
   ],
   "source": [
    "Configuration1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: 1 input layer: 3 hidden Layer : 1 output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Configuration2():\n",
    "    # confirm scale of pixels\n",
    "    print('Train min=%.3f, max=%.3f' % (trainX.min(), trainX.max()))\n",
    "    print('Test min=%.3f, max=%.3f' % (testX.min(), testX.max()))\n",
    "    # create generator (1.0/255.0 = 0.003921568627451)\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    test_iterator = datagen.flow(testX, testY, batch_size=64)\n",
    "    print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "    # confirm the scaling works\n",
    "    batchX, batchy = train_iterator.next()\n",
    "    print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels),\n",
    "                    kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit model with generator\n",
    "    model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min=0.000, max=255.000\n",
      "Test min=0.000, max=255.000\n",
      "Batches train=938, test=157\n",
      "Batch shape=(64, 28, 28, 1), min=0.000, max=1.000\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 18s 18ms/step - loss: 0.7685 - accuracy: 0.7245\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.3962 - accuracy: 0.8555\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.3319 - accuracy: 0.8796\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2930 - accuracy: 0.8914\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2695 - accuracy: 0.9001\n",
      "Test Accuracy: 88.470\n"
     ]
    }
   ],
   "source": [
    "Configuration2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSprop Optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSpropOptimization():\n",
    "    # confirm scale of pixels\n",
    "    print('Train min=%.3f, max=%.3f' % (trainX.min(), trainX.max()))\n",
    "    print('Test min=%.3f, max=%.3f' % (testX.min(), testX.max()))\n",
    "    # create generator (1.0/255.0 = 0.003921568627451)\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    test_iterator = datagen.flow(testX, testY, batch_size=64)\n",
    "    print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "    # confirm the scaling works\n",
    "    batchX, batchy = train_iterator.next()\n",
    "    print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels),\n",
    "                    kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit model with generator\n",
    "    model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min=0.000, max=255.000\n",
      "Test min=0.000, max=255.000\n",
      "Batches train=938, test=157\n",
      "Batch shape=(64, 28, 28, 1), min=0.000, max=1.000\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.6533 - accuracy: 0.7685\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.3110 - accuracy: 0.8862\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.2621 - accuracy: 0.9032\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.2271 - accuracy: 0.9178\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.2044 - accuracy: 0.9240\n",
      "Test Accuracy: 90.490\n"
     ]
    }
   ],
   "source": [
    "RMSpropOptimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adadelta Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adadeltatimization():\n",
    "    # confirm scale of pixels\n",
    "    print('Train min=%.3f, max=%.3f' % (trainX.min(), trainX.max()))\n",
    "    print('Test min=%.3f, max=%.3f' % (testX.min(), testX.max()))\n",
    "    # create generator (1.0/255.0 = 0.003921568627451)\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    test_iterator = datagen.flow(testX, testY, batch_size=64)\n",
    "    print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "    # confirm the scaling works\n",
    "    batchX, batchy = train_iterator.next()\n",
    "    print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels),\n",
    "                    kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='Adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit model with generator\n",
    "    model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min=0.000, max=255.000\n",
      "Test min=0.000, max=255.000\n",
      "Batches train=938, test=157\n",
      "Batch shape=(64, 28, 28, 1), min=0.000, max=1.000\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 2.5760 - accuracy: 0.18050s - loss: 2.5784 - accuracy: 0.\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 1.8561 - accuracy: 0.3937\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 1.5922 - accuracy: 0.5324\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 1.3887 - accuracy: 0.6075\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 1.2341 - accuracy: 0.6546\n",
      "Test Accuracy: 66.860\n"
     ]
    }
   ],
   "source": [
    "Adadeltatimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeLU Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeLUActivation():\n",
    "    # confirm scale of pixels\n",
    "    print('Train min=%.3f, max=%.3f' % (trainX.min(), trainX.max()))\n",
    "    print('Test min=%.3f, max=%.3f' % (testX.min(), testX.max()))\n",
    "    # create generator (1.0/255.0 = 0.003921568627451)\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    test_iterator = datagen.flow(testX, testY, batch_size=64)\n",
    "    print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "    # confirm the scaling works\n",
    "    batchX, batchy = train_iterator.next()\n",
    "    print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='selu', input_shape=(width, height, channels),\n",
    "                    kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='selu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='selu',kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit model with generator\n",
    "    model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min=0.000, max=255.000\n",
      "Test min=0.000, max=255.000\n",
      "Batches train=938, test=157\n",
      "Batch shape=(64, 28, 28, 1), min=0.000, max=1.000\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.5710 - accuracy: 0.7995\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2980 - accuracy: 0.8904\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2494 - accuracy: 0.9107\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2116 - accuracy: 0.9224\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.1826 - accuracy: 0.9323\n",
      "Test Accuracy: 89.730\n"
     ]
    }
   ],
   "source": [
    "SeLUActivation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PReLU Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import PReLU\n",
    "def PReLUActivation():\n",
    "    # confirm scale of pixels\n",
    "    print('Train min=%.3f, max=%.3f' % (trainX.min(), trainX.max()))\n",
    "    print('Test min=%.3f, max=%.3f' % (testX.min(), testX.max()))\n",
    "    # create generator (1.0/255.0 = 0.003921568627451)\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    test_iterator = datagen.flow(testX, testY, batch_size=64)\n",
    "    print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "    # confirm the scaling works\n",
    "    batchX, batchy = train_iterator.next()\n",
    "    print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation=PReLU(), input_shape=(width, height, channels),\n",
    "                    kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation=PReLU(),kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=PReLU(),kernel_initializer = tf.keras.initializers.HeNormal()))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit model with generator\n",
    "    model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min=0.000, max=255.000\n",
      "Test min=0.000, max=255.000\n",
      "Batches train=938, test=157\n",
      "Batch shape=(64, 28, 28, 1), min=0.000, max=1.000\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.6309 - accuracy: 0.7777\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.3266 - accuracy: 0.8822\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.2757 - accuracy: 0.8982\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 31s 34ms/step - loss: 0.2429 - accuracy: 0.9121\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.2132 - accuracy: 0.9229\n",
      "Test Accuracy: 90.200\n"
     ]
    }
   ],
   "source": [
    "PReLUActivation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L1Regularization():\n",
    "    # confirm scale of pixels\n",
    "    print('Train min=%.3f, max=%.3f' % (trainX.min(), trainX.max()))\n",
    "    print('Test min=%.3f, max=%.3f' % (testX.min(), testX.max()))\n",
    "    # create generator (1.0/255.0 = 0.003921568627451)\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    test_iterator = datagen.flow(testX, testY, batch_size=64)\n",
    "    print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "    # confirm the scaling works\n",
    "    batchX, batchy = train_iterator.next()\n",
    "    print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='selu', input_shape=(width, height, channels),\n",
    "                    kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                     kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='selu',kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                     kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='selu',kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                    kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit model with generator\n",
    "    model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min=0.000, max=255.000\n",
      "Test min=0.000, max=255.000\n",
      "Batches train=938, test=157\n",
      "Batch shape=(64, 28, 28, 1), min=0.000, max=1.000\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 12.0934 - accuracy: 0.7181\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 1.2401 - accuracy: 0.7817\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 1.0683 - accuracy: 0.7935\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 1.0352 - accuracy: 0.8003\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 17s 19ms/step - loss: 1.0038 - accuracy: 0.8038\n",
      "Test Accuracy: 80.250\n"
     ]
    }
   ],
   "source": [
    "L1Regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2Regularization():\n",
    "    # confirm scale of pixels\n",
    "    print('Train min=%.3f, max=%.3f' % (trainX.min(), trainX.max()))\n",
    "    print('Test min=%.3f, max=%.3f' % (testX.min(), testX.max()))\n",
    "    # create generator (1.0/255.0 = 0.003921568627451)\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    test_iterator = datagen.flow(testX, testY, batch_size=64)\n",
    "    print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "    # confirm the scaling works\n",
    "    batchX, batchy = train_iterator.next()\n",
    "    print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels),\n",
    "                    kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu',kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit model with generator\n",
    "    model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min=0.000, max=255.000\n",
      "Test min=0.000, max=255.000\n",
      "Batches train=938, test=157\n",
      "Batch shape=(64, 28, 28, 1), min=0.000, max=1.000\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 17s 17ms/step - loss: 2.1105 - accuracy: 0.7528\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.7548 - accuracy: 0.8245\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.6756 - accuracy: 0.8281\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.6412 - accuracy: 0.8336\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.6194 - accuracy: 0.8369\n",
      "Test Accuracy: 83.200\n"
     ]
    }
   ],
   "source": [
    "L2Regularization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
